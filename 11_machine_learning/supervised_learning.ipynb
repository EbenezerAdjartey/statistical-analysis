{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "k01",
   "metadata": {},
   "source": [
    "# 11 â€” Supervised Machine Learning\n",
    "**Author:** Ebenezer Adjartey\n\n",
    "Covers: Linear/Ridge/Lasso regression, Logistic regression, Decision Trees, Random Forests, Gradient Boosting (XGBoost), SVM, KNN, Neural Networks (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "id": "k02",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification, make_regression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_text\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n                               roc_auc_score, mean_squared_error, root_mean_squared_error,\n                               r2_score)\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\nsns.set_theme(style='whitegrid')\nprint('Libraries loaded.')"
  },
  {
   "cell_type": "markdown",
   "id": "k03",
   "metadata": {},
   "source": [
    "## 1. Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "k04",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Classification dataset\n",
    "X_cls, y_cls = make_classification(n_samples=600, n_features=10, n_informative=6,\n",
    "                                    n_redundant=2, random_state=42)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_cls, y_cls, test_size=0.25, random_state=42)\n",
    "\n",
    "# Regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=10, noise=20, random_state=42)\n",
    "Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(X_reg, y_reg, test_size=0.25, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_sc = scaler.fit_transform(X_tr)\n",
    "X_te_sc = scaler.transform(X_te)\n",
    "Xr_tr_sc = scaler.fit_transform(Xr_tr)\n",
    "Xr_te_sc = scaler.transform(Xr_te)\n",
    "\n",
    "print(f'Classification: {X_tr.shape[0]} train, {X_te.shape[0]} test')\n",
    "print(f'Regression:     {Xr_tr.shape[0]} train, {Xr_te.shape[0]} test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k05",
   "metadata": {},
   "source": [
    "## 2. Linear Regression (OLS, Ridge, Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "id": "k06",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "reg_results = {}\n\nfor name, model in [('OLS',   LinearRegression()),\n                     ('Ridge', Ridge(alpha=1.0)),\n                     ('Lasso', Lasso(alpha=0.5))]:\n    model.fit(Xr_tr_sc, yr_tr)\n    pred = model.predict(Xr_te_sc)\n    rmse = root_mean_squared_error(yr_te, pred)\n    r2   = r2_score(yr_te, pred)\n    reg_results[name] = {'RMSE': round(rmse,3), 'R2': round(r2,3)}\n    print(f'{name:6}: RMSE={rmse:.3f}, R2={r2:.3f}, coefs_nonzero={np.sum(model.coef_!=0)}')\n\nprint('\\nLasso performs automatic feature selection (zeros out coefficients)')"
  },
  {
   "cell_type": "markdown",
   "id": "k07",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "id": "k08",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_tr_sc, y_tr)\n",
    "y_pred_lr = lr.predict(X_te_sc)\n",
    "y_prob_lr = lr.predict_proba(X_te_sc)[:,1]\n",
    "print(f'Logistic Regression Accuracy: {accuracy_score(y_te, y_pred_lr):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_te, y_prob_lr):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_te, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k09",
   "metadata": {},
   "source": [
    "## 4. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "id": "k10",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt.fit(X_tr, y_tr)\n",
    "y_pred_dt = dt.predict(X_te)\n",
    "print(f'Decision Tree Accuracy: {accuracy_score(y_te, y_pred_dt):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_te, dt.predict_proba(X_te)[:,1]):.4f}')\n",
    "print(f'Tree depth: {dt.get_depth()}')\n",
    "print(f'Feature importances: {dt.feature_importances_.round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k11",
   "metadata": {},
   "source": [
    "## 5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "id": "k12",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_tr, y_tr)\n",
    "y_pred_rf = rf.predict(X_te)\n",
    "y_prob_rf = rf.predict_proba(X_te)[:,1]\n",
    "print(f'Random Forest Accuracy: {accuracy_score(y_te, y_pred_rf):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_te, y_prob_rf):.4f}')\n",
    "\n",
    "# Feature importance\n",
    "fi = pd.Series(rf.feature_importances_, name='Importance').sort_values(ascending=False)\n",
    "print('\\nFeature Importances (sorted):')\n",
    "print(fi.round(4).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k13",
   "metadata": {},
   "source": [
    "## 6. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "id": "k14",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_tr, y_tr)\n",
    "y_pred_gb = gb.predict(X_te)\n",
    "y_prob_gb = gb.predict_proba(X_te)[:,1]\n",
    "print(f'GradientBoosting Accuracy: {accuracy_score(y_te, y_pred_gb):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_te, y_prob_gb):.4f}')\n",
    "\n",
    "# Try XGBoost if available\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "                         use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    print(f'XGBoost Accuracy: {accuracy_score(y_te, xgb.predict(X_te)):.4f}')\n",
    "    print(f'XGBoost AUC: {roc_auc_score(y_te, xgb.predict_proba(X_te)[:,1]):.4f}')\n",
    "except ImportError:\n",
    "    print('XGBoost not installed (pip install xgboost)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k15",
   "metadata": {},
   "source": [
    "## 7. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "id": "k16",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "svm = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
    "svm.fit(X_tr_sc, y_tr)\n",
    "y_pred_svm = svm.predict(X_te_sc)\n",
    "print(f'SVM (RBF) Accuracy: {accuracy_score(y_te, y_pred_svm):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_te, svm.predict_proba(X_te_sc)[:,1]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k17",
   "metadata": {},
   "source": [
    "## 8. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "id": "k18",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_tr_sc, y_tr)\n",
    "y_pred_knn = knn.predict(X_te_sc)\n",
    "print(f'KNN (k=5) Accuracy: {accuracy_score(y_te, y_pred_knn):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_te, knn.predict_proba(X_te_sc)[:,1]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k19",
   "metadata": {},
   "source": [
    "## 9. Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "id": "k20",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(64,32), activation='relu',\n",
    "                     max_iter=500, random_state=42)\n",
    "mlp.fit(X_tr_sc, y_tr)\n",
    "y_pred_mlp = mlp.predict(X_te_sc)\n",
    "print(f'MLP Accuracy: {accuracy_score(y_te, y_pred_mlp):.4f}')\n",
    "print(f'AUC: {roc_auc_score(y_te, mlp.predict_proba(X_te_sc)[:,1]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k21",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "id": "k22",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "models_cls = {\n",
    "    'Logistic Reg': (y_pred_lr, y_prob_lr),\n",
    "    'Decision Tree': (y_pred_dt, dt.predict_proba(X_te)[:,1]),\n",
    "    'Random Forest': (y_pred_rf, y_prob_rf),\n",
    "    'Gradient Boost': (y_pred_gb, y_prob_gb),\n",
    "    'SVM':           (y_pred_svm, svm.predict_proba(X_te_sc)[:,1]),\n",
    "    'KNN':           (y_pred_knn, knn.predict_proba(X_te_sc)[:,1]),\n",
    "    'MLP':           (y_pred_mlp, mlp.predict_proba(X_te_sc)[:,1]),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, (pred, prob) in models_cls.items():\n",
    "    results.append({'Model':name,\n",
    "                    'Accuracy': round(accuracy_score(y_te, pred),4),\n",
    "                    'AUC':      round(roc_auc_score(y_te, prob),4)})\n",
    "\n",
    "comp_df = pd.DataFrame(results).sort_values('AUC', ascending=False)\n",
    "print('Model Comparison:')\n",
    "print(comp_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "comp_df.set_index('Model')[['Accuracy','AUC']].plot(kind='bar', ax=ax)\n",
    "ax.set_title('Classification Model Comparison'); ax.set_ylim(.5, 1)\n",
    "plt.xticks(rotation=30, ha='right'); plt.tight_layout()\n",
    "os.makedirs('11_machine_learning', exist_ok=True)\n",
    "plt.savefig('11_machine_learning/model_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show(); print('Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k23",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n",
    "- **Regularization** (Ridge, Lasso): prevents overfitting; Lasso does feature selection\n",
    "- **Ensemble methods** (RF, GB): generally outperform single models\n",
    "- **SVM**: effective in high-dimensional spaces; kernel trick for non-linearity\n",
    "- **Neural networks**: flexible but need more data and tuning\n",
    "- **Scale features** for distance-based methods (SVM, KNN, MLP)\n"
   ]
  }
 ]
}