{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "l01",
   "metadata": {},
   "source": [
    "# 12 \u2014 Model Evaluation & Testing\n",
    "**Author:** Ebenezer Adjartey\n\n",
    "Covers: train/test split, k-fold CV, confusion matrix metrics, ROC/AUC, regression metrics, hyperparameter tuning, SHAP values, learning curves."
   ]
  },
  {
   "cell_type": "code",
   "id": "l02",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import (train_test_split, KFold, StratifiedKFold,\n",
    "                                     cross_val_score, GridSearchCV, RandomizedSearchCV,\n",
    "                                     learning_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
    "                               classification_report, accuracy_score,\n",
    "                               precision_score, recall_score, f1_score,\n",
    "                               roc_curve, roc_auc_score,\n",
    "                               mean_squared_error, mean_absolute_error, r2_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "sns.set_theme(style='whitegrid')\n",
    "print('Libraries loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l03",
   "metadata": {},
   "source": [
    "## 1. Train/Test Split & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "l04",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "X, y = make_classification(n_samples=800, n_features=15, n_informative=8, random_state=42)\n",
    "\n",
    "# Simple train/test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f'Train: {X_tr.shape[0]}  Test: {X_te.shape[0]}')\n",
    "print(f'Class balance (train): {np.bincount(y_tr)}')\n",
    "\n",
    "# 5-fold stratified cross-validation\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv_scores = cross_val_score(clf, X, y, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "print(f'\\n5-fold CV AUC scores: {cv_scores.round(4)}')\n",
    "print(f'Mean AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l05",
   "metadata": {},
   "source": [
    "## 2. Confusion Matrix & Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "l06",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "clf.fit(X_tr, y_tr)\n",
    "y_pred = clf.predict(X_te)\n",
    "y_prob = clf.predict_proba(X_te)[:,1]\n",
    "\n",
    "cm = confusion_matrix(y_te, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "print(f'\\nTN={tn}  FP={fp}  FN={fn}  TP={tp}')\n",
    "print(f'Accuracy:  {accuracy_score(y_te,y_pred):.4f}')\n",
    "print(f'Precision: {precision_score(y_te,y_pred):.4f}  (TP/(TP+FP))')\n",
    "print(f'Recall:    {recall_score(y_te,y_pred):.4f}   (TP/(TP+FN))')\n",
    "print(f'F1-Score:  {f1_score(y_te,y_pred):.4f}   (harmonic mean of P and R)')\n",
    "print(f'Specificity: {tn/(tn+fp):.4f}  (TN/(TN+FP))')\n",
    "print(f'AUC:       {roc_auc_score(y_te,y_prob):.4f}')\n",
    "print('\\nFull Report:')\n",
    "print(classification_report(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l07",
   "metadata": {},
   "source": [
    "## 3. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "id": "l08",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Multiple classifiers\n",
    "models_eval = {\n",
    "    'Random Forest':   RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boost':  GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Reg':    LogisticRegression(max_iter=1000),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "for name, m in models_eval.items():\n",
    "    m.fit(X_tr, y_tr)\n",
    "    probs = m.predict_proba(X_te)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_te, probs)\n",
    "    auc = roc_auc_score(y_te, probs)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f'{name} (AUC={auc:.3f})')\n",
    "\n",
    "ax.plot([0,1],[0,1],'k--', label='Random (AUC=0.5)')\n",
    "ax.set_title('ROC Curves'); ax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')\n",
    "ax.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "os.makedirs('12_model_evaluation', exist_ok=True)\n",
    "plt.savefig('12_model_evaluation/roc_curves.png', dpi=100, bbox_inches='tight')\n",
    "plt.show(); print('Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l09",
   "metadata": {},
   "source": [
    "## 4. Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "l10",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_reg, y_reg = make_regression(n_samples=500, n_features=10, noise=30, random_state=42)\n",
    "Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(Xr_tr, yr_tr)\n",
    "yr_pred = ridge.predict(Xr_te)\n",
    "\n",
    "mae  = mean_absolute_error(yr_te, yr_pred)\n",
    "rmse = mean_squared_error(yr_te, yr_pred, squared=False)\n",
    "r2   = r2_score(yr_te, yr_pred)\n",
    "mape = np.mean(np.abs((yr_te - yr_pred) / (yr_te + 1e-8))) * 100\n",
    "\n",
    "print(f'MAE:  {mae:.3f}   (mean absolute error)')\n",
    "print(f'RMSE: {rmse:.3f}  (root mean squared error; penalizes large errors)')\n",
    "print(f'R2:   {r2:.4f}  (proportion of variance explained)')\n",
    "print(f'MAPE: {mape:.2f}%  (mean absolute percentage error)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l11",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "l12",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth':    [3, 5, 10],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid, cv=3, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_tr, y_tr)\n",
    "print('GridSearchCV best params:', grid_search.best_params_)\n",
    "print(f'Best CV AUC: {grid_search.best_score_:.4f}')\n",
    "\n",
    "# RandomizedSearchCV (faster for large spaces)\n",
    "from scipy.stats import randint\n",
    "param_dist = {'n_estimators': randint(50,200), 'max_depth': randint(3,15)}\n",
    "rand_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_dist, n_iter=10, cv=3, scoring='roc_auc', random_state=42\n",
    ")\n",
    "rand_search.fit(X_tr, y_tr)\n",
    "print('\\nRandomizedSearchCV best params:', rand_search.best_params_)\n",
    "print(f'Best CV AUC: {rand_search.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l13",
   "metadata": {},
   "source": [
    "## 6. Feature Importance & SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "id": "l14",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature importance from RF\n",
    "best_rf = grid_search.best_estimator_\n",
    "fi_df = pd.DataFrame({'Feature': [f'X{i}' for i in range(X.shape[1])],\n",
    "                       'Importance': best_rf.feature_importances_})\n",
    "fi_df = fi_df.sort_values('Importance', ascending=False)\n",
    "print('Feature Importances:')\n",
    "print(fi_df.round(4).to_string(index=False))\n",
    "\n",
    "# SHAP values (if installed)\n",
    "try:\n",
    "    import shap\n",
    "    explainer = shap.TreeExplainer(best_rf)\n",
    "    shap_values = explainer.shap_values(X_te[:100])\n",
    "    print('\\nSHAP values computed (shape):', np.array(shap_values).shape)\n",
    "    # SHAP summary plot\n",
    "    shap.summary_plot(shap_values[1], X_te[:100],\n",
    "                      feature_names=[f'X{i}' for i in range(X.shape[1])],\n",
    "                      show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('12_model_evaluation/shap_summary.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show(); print('SHAP plot saved.')\n",
    "except ImportError:\n",
    "    print('SHAP not installed (pip install shap)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l15",
   "metadata": {},
   "source": [
    "## 7. Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "id": "l16",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    X, y, cv=5, scoring='roc_auc',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std  = train_scores.std(axis=1)\n",
    "val_mean   = val_scores.mean(axis=1)\n",
    "val_std    = val_scores.std(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Learning curve\n",
    "axes[0].plot(train_sizes, train_mean, 'b-o', label='Training AUC')\n",
    "axes[0].fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=.1, color='b')\n",
    "axes[0].plot(train_sizes, val_mean, 'r-o', label='Validation AUC')\n",
    "axes[0].fill_between(train_sizes, val_mean-val_std, val_mean+val_std, alpha=.1, color='r')\n",
    "axes[0].set_title('Learning Curve')\n",
    "axes[0].set_xlabel('Training Size'); axes[0].set_ylabel('AUC')\n",
    "axes[0].legend()\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "best_rf.fit(X_tr, y_tr)\n",
    "cm = confusion_matrix(y_te, best_rf.predict(X_te))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title(f'Confusion Matrix (AUC={roc_auc_score(y_te,best_rf.predict_proba(X_te)[:,1]):.3f})')\n",
    "axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('12_model_evaluation/learning_curves.png', dpi=100, bbox_inches='tight')\n",
    "plt.show(); print('Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l17",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n",
    "- **Cross-validation** gives a more honest estimate of generalization than a single train/test split\n",
    "- **Precision vs Recall trade-off**: choose threshold based on the cost of FP vs FN\n",
    "- **AUC**: threshold-independent measure of discrimination ability\n",
    "- **GridSearchCV**: exhaustive search; RandomizedSearch is faster for large spaces\n",
    "- **SHAP**: model-agnostic interpretability; shows feature impact per prediction\n",
    "- **Learning curves**: diagnose overfitting (high train, low val) vs underfitting (both low)\n"
   ]
  }
 ]
}