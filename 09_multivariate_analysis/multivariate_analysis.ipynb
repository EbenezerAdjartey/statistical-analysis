{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "i01",
   "metadata": {},
   "source": [
    "# 09 â€” Multivariate Analysis\n",
    "**Author:** Ebenezer Adjartey\n\n",
    "Covers: PCA, Exploratory Factor Analysis (EFA), k-means and hierarchical clustering, Linear Discriminant Analysis (LDA), MANOVA, Canonical Correlation Analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "i02",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import silhouette_score\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "sns.set_theme(style='whitegrid')\n",
    "print('Libraries loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i03",
   "metadata": {},
   "source": [
    "## 1. Synthetic Dataset (8 Variables)"
   ]
  },
  {
   "cell_type": "code",
   "id": "i04",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "n = 200\n",
    "# Create correlated variables with 3 underlying factors\n",
    "f1 = np.random.normal(0,1,n)  # verbal ability\n",
    "f2 = np.random.normal(0,1,n)  # quantitative ability\n",
    "f3 = np.random.normal(0,1,n)  # memory\n",
    "\n",
    "X = np.column_stack([\n",
    "    0.8*f1 + 0.1*f2 + np.random.normal(0,.3,n),  # reading\n",
    "    0.7*f1 + 0.2*f2 + np.random.normal(0,.3,n),  # vocabulary\n",
    "    0.1*f1 + 0.8*f2 + np.random.normal(0,.3,n),  # math\n",
    "    0.2*f1 + 0.9*f2 + np.random.normal(0,.3,n),  # statistics\n",
    "    0.1*f1 + 0.1*f2 + 0.8*f3 + np.random.normal(0,.3,n),  # memory1\n",
    "    0.2*f1 + 0.1*f2 + 0.7*f3 + np.random.normal(0,.3,n),  # memory2\n",
    "    np.random.normal(0,1,n),  # noise1\n",
    "    np.random.normal(0,1,n),  # noise2\n",
    "])\n",
    "cols = ['reading','vocabulary','math','statistics','memory1','memory2','noise1','noise2']\n",
    "df   = pd.DataFrame(X, columns=cols)\n",
    "\n",
    "# True group labels for LDA/MANOVA\n",
    "group = pd.cut(f1 + f2, bins=3, labels=[0,1,2]).astype(int)\n",
    "df['group'] = group\n",
    "\n",
    "print(df.head())\n",
    "print('\\nCorrelation matrix:')\n",
    "print(df[cols].corr().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i05",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "id": "i06",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_sc = scaler.fit_transform(df[cols])\n",
    "\n",
    "# PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_sc)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "ev_df = pd.DataFrame({\n",
    "    'PC': [f'PC{i+1}' for i in range(len(explained))],\n",
    "    'Eigenvalue':        pca.explained_variance_.round(4),\n",
    "    'Variance_%':        (explained*100).round(2),\n",
    "    'Cumulative_%':      (cumulative*100).round(2)\n",
    "})\n",
    "print('Explained Variance Table:')\n",
    "print(ev_df.to_string(index=False))\n",
    "\n",
    "n_components = np.argmax(cumulative >= 0.80) + 1\n",
    "print(f'\\nComponents needed for 80% variance: {n_components}')\n",
    "\n",
    "# Loadings\n",
    "loadings = pd.DataFrame(pca.components_[:4].T, index=cols,\n",
    "                         columns=[f'PC{i+1}' for i in range(4)]).round(3)\n",
    "print('\\nPC Loadings (first 4 PCs):')\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "id": "i07",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# PCA visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1,9), pca.explained_variance_, color='steelblue')\n",
    "axes[0].axhline(1, color='red', linestyle='--', label='Kaiser criterion (eigenvalue=1)')\n",
    "axes[0].set_title('Scree Plot'); axes[0].set_xlabel('PC'); axes[0].set_ylabel('Eigenvalue')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# Biplot\n",
    "pcs = pca.transform(X_sc)\n",
    "axes[1].scatter(pcs[:,0], pcs[:,1], c=df['group'], cmap='Set1', alpha=.5, s=20)\n",
    "for i, col in enumerate(cols):\n",
    "    axes[1].arrow(0,0, pca.components_[0,i]*2, pca.components_[1,i]*2,\n",
    "                  head_width=.05, color='red', alpha=.7)\n",
    "    axes[1].text(pca.components_[0,i]*2.1, pca.components_[1,i]*2.1, col, fontsize=7)\n",
    "axes[1].set_title('PCA Biplot (PC1 vs PC2)')\n",
    "axes[1].set_xlabel(f'PC1 ({explained[0]*100:.1f}%)')\n",
    "axes[1].set_ylabel(f'PC2 ({explained[1]*100:.1f}%)')\n",
    "\n",
    "# Cumulative variance\n",
    "axes[2].plot(range(1,9), cumulative*100, 'bo-', lw=2)\n",
    "axes[2].axhline(80, color='red', linestyle='--', label='80% threshold')\n",
    "axes[2].set_title('Cumulative Explained Variance')\n",
    "axes[2].set_xlabel('Number of PCs'); axes[2].set_ylabel('%')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('09_multivariate_analysis', exist_ok=True)\n",
    "plt.savefig('09_multivariate_analysis/pca_plots.png', dpi=100, bbox_inches='tight')\n",
    "plt.show(); print('Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i08",
   "metadata": {},
   "source": [
    "## 3. Exploratory Factor Analysis (EFA)"
   ]
  },
  {
   "cell_type": "code",
   "id": "i09",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Factor Analysis with 3 factors\n",
    "fa = FactorAnalysis(n_components=3, random_state=42)\n",
    "fa.fit(X_sc)\n",
    "\n",
    "fa_loadings = pd.DataFrame(fa.components_.T, index=cols,\n",
    "                             columns=[f'Factor{i+1}' for i in range(3)]).round(3)\n",
    "print('Factor Loadings (|loading| > 0.4 = strong):')\n",
    "print(fa_loadings)\n",
    "\n",
    "# Uniqueness (1 - communality)\n",
    "communality = 1 - fa.noise_variance_\n",
    "print('\\nCommunalities (proportion of variance explained):')\n",
    "for col, comm in zip(cols, communality):\n",
    "    print(f'  {col:12}: {comm:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i10",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "id": "i11",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Determine optimal k using elbow + silhouette\n",
    "inertias, sil_scores = [], []\n",
    "k_range = range(2, 9)\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_sc)\n",
    "    inertias.append(km.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_sc, labels))\n",
    "\n",
    "best_k = list(k_range)[np.argmax(sil_scores)]\n",
    "print(f'Best k by silhouette score: {best_k}')\n",
    "\n",
    "# Final k-means with best k\n",
    "km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "df['km_cluster'] = km_final.fit_predict(X_sc)\n",
    "print('\\nCluster sizes:')\n",
    "print(df['km_cluster'].value_counts().sort_index())\n",
    "\n",
    "print('\\nCluster centers (standardized):')\n",
    "centers = pd.DataFrame(km_final.cluster_centers_, columns=cols).round(3)\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i12",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "id": "i13",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ward linkage dendrogram (on subset of 50 obs for clarity)\n",
    "Z = linkage(X_sc[:50], method='ward')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "dendrogram(Z, ax=ax, truncate_mode='lastp', p=15,\n",
    "            show_leaf_counts=True, leaf_rotation=90)\n",
    "ax.set_title('Hierarchical Clustering Dendrogram (Ward)')\n",
    "ax.set_xlabel('Observations'); ax.set_ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('09_multivariate_analysis/dendrogram.png', dpi=100, bbox_inches='tight')\n",
    "plt.show(); print('Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i14",
   "metadata": {},
   "source": [
    "## 6. Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "id": "i15",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_sc, df['group'])\n",
    "\n",
    "print('LDA Explained variance ratio:', lda.explained_variance_ratio_.round(4))\n",
    "print('Coefficients (LD1, LD2):')\n",
    "coef_df = pd.DataFrame(lda.coef_.T, index=cols,\n",
    "                         columns=[f'LD{i+1}' for i in range(lda.coef_.shape[0])]).round(4)\n",
    "print(coef_df)\n",
    "\n",
    "# LDA plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "scatter = ax.scatter(X_lda[:,0], X_lda[:,1], c=df['group'], cmap='Set1', alpha=.6, s=30)\n",
    "ax.set_title('LDA: First Two Discriminant Functions')\n",
    "ax.set_xlabel('LD1'); ax.set_ylabel('LD2')\n",
    "plt.colorbar(scatter, label='Group')\n",
    "plt.tight_layout()\n",
    "plt.savefig('09_multivariate_analysis/lda_plot.png', dpi=100, bbox_inches='tight')\n",
    "plt.show(); print('Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i16",
   "metadata": {},
   "source": [
    "## 7. MANOVA"
   ]
  },
  {
   "cell_type": "code",
   "id": "i17",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "df_manova = df[['reading','vocabulary','math','statistics','group']].copy()\n",
    "manova = MANOVA.from_formula('reading + vocabulary + math + statistics ~ C(group)',\n",
    "                               data=df_manova)\n",
    "result = manova.mv_test()\n",
    "print('MANOVA Results:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i18",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n",
    "- **PCA**: reduces dimensionality; components are uncorrelated linear combinations\n",
    "- **EFA**: identifies latent factors; loadings show variable-factor relationships\n",
    "- **K-means**: minimizes within-cluster variance; use elbow/silhouette to choose k\n",
    "- **Hierarchical**: no need to pre-specify k; dendrogram shows structure\n",
    "- **LDA**: maximizes between-group vs within-group variance; good for classification\n",
    "- **MANOVA**: multivariate extension of ANOVA for multiple dependent variables\n"
   ]
  }
 ]
}